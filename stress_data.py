# -*- coding: utf-8 -*-
"""Stress Data.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qR4am8fcw5PKabgB45bzMC1_vX0QMT4-

# **Finding and using the correlation between different health metrics to predict a patients stress level based on their lifestyle and what changes they can make to improve their health.**

# **Loading and Preprocessing**
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import sklearn
import networkx as nx
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score
from sklearn.metrics.pairwise import cosine_similarity
from scipy.sparse import csr_matrix
from sklearn.ensemble import RandomForestClassifier
df = pd.read_csv('stress_detection_data.csv')
# Remove any NaN values
df.dropna(inplace=True)
#Remove any duplicate values
df.drop_duplicates(inplace=True)
# As there are no text columns, text cleaning will not be necessary
df.head()
print(df.info())

"""# **Sorting the users into different job fields**"""

print(df['Occupation'].unique()) # find all unique jobs in dataset

#Sort all jobs into different fields
technology_it = [
    "Software Engineer", "Software Developer", "IT Manager", "Web Developer",
    "IT Consultant", "IT Specialist", "Network Engineer", "Database Administrator",
    "Software Architect", "Network Administrator", "Software Tester", "UX Designer",
    "Data Engineer", "Developer", "IT Support Specialist", "IT Support",
    "Programmer", "Systems Analyst", "Cybersecurity Specialist"
]

healthcare_medical = [
    "Doctor", "Nurse", "Physician", "Pharmacist", "Nutritionist", "Psychologist",
    "Veterinarian", "Physiotherapist", "Dentist", "Surgeon", "Nurse Practitioner",
    "Medical Assistant", "Healthcare Assistant", "Therapist", "Laboratory Technician",
    "Nutritional Specialist"
]

business_management = [
    "Business Owner", "Business Consultant", "Business Analyst", "Manager",
    "Operations Manager", "Project Manager", "Product Manager", "HR Manager",
    "HR Specialist", "Human Resources", "Human Resources Manager", "Account Manager",
    "CEO", "Executive Director", "Retail Manager", "Bank Manager", "Entrepreneur",
    "Shopkeeper", "Restaurant Manager", "Financial Advisor", "Financial Planner",
    "Insurance Agent", "Real Estate Agent", "Sales Manager", "Sales Executive",
    "Sales Representative", "Marketing Director", "Advertising Manager",
    "Brand Manager", "Public Relations Specialist"
]

engineering_construction = [
    "Civil Engineer", "Engineer", "Mechanical Engineer", "Electrical Engineer",
    "Construction Worker", "Construction Manager", "Construction Engineer",
    "Architect", "Interior Designer", "Electrician", "Plumber", "Carpenter",
    "Technician", "Electrical Technician"
]

creative_design = [
    "Graphic Designer", "Content Writer", "Content Creator", "Copywriter",
    "Photographer", "Artist", "Musician", "Fashion Designer", "Designer",
    "Potter", "Handicrafts Maker", "Painter", "Tailor", "Seamstress", "Weaver",
    "Editor", "Content Strategist", "Advertising Executive"
]

education_research = [
    "Teacher", "Primary School Teacher", "Research Assistant", "Research Scientist",
    "Scientist", "Researcher", "Physicist", "Biologist", "Librarian",
    "Social Worker", "Professor", "Academic Advisor"
]

marketing_communications = [
    "Marketing Manager", "Marketing Specialist", "Marketing Executive",
    "Digital Marketer", "SEO Specialist", "Journalist", "Public Relations Specialist",
    "Event Planner", "Event Coordinator", "Event Manager", "Copywriter",
    "Content Marketer", "Social Media Manager"
]

finance_accounting = [
    "Accountant", "Financial Analyst", "Banker", "Financial Planner",
    "Investment Analyst", "Tax Consultant", "Auditor"
]

administrative_support = [
    "Receptionist", "Secretary", "Customer Support", "Personal Assistant",
    "Administrative Assistant", "Office Manager", "Data Entry Clerk"
]

skilled_trades_labor = [
    "Chef", "Baker", "Bartender", "Hair Stylist", "Cobbler", "Mechanic",
    "Blacksmith", "Farmer", "Fisherwoman", "Truck Driver", "Delivery Driver",
    "Courier", "Janitor", "Cleaner", "Warehouse Worker", "Factory Worker",
    "Security Guard", "Security Officer", "Police Officer", "Firefighter",
    "Pilot", "Bus Driver", "Taxi Driver", "Driver", "Nanny", "Waitress",
    "Salesperson", "Street Vendor", "Vegetable Vendor", "Flower Seller",
    "Bakery Owner"
]

freelance_gig = [
    "Freelancer", "Consultant", "Independent Contractor", "Gig Worker"
]

other = [
    "Retired", "Student", "Unemployed", "Volunteer"
]
#Put all the job categories into a dictionary
occupation_categories = {
    "Technology/IT": technology_it,
    "Healthcare/Medical": healthcare_medical,
    "Business/Management": business_management,
    "Engineering/Construction": engineering_construction,
    "Creative/Design": creative_design,
    "Education/Research": education_research,
    "Marketing/Communications": marketing_communications,
    "Finance/Accounting": finance_accounting,
    "Administrative/Support": administrative_support,
    "Skilled Trades/Labor": skilled_trades_labor,
    "Freelance/Gig": freelance_gig,
    "Other": other
}
#Create a new column to distinguish the person field of work
# Use a function to handle occupations
def get_field_of_work(occupation):
    for category, occupations in occupation_categories.items():
        if occupation in occupations:
            return category
    return "Other"  # Assign "Other" for unknown occupations
df['Field_of_work'] = df['Occupation'].apply(get_field_of_work)

"""# **How do stress patterns vary across different fields of work, and which professional fields exhibit similar stress profiles?**"""

class StressFieldAnalyzer:
    def __init__(self, df):
        self.df = df

    def create_sparse_matrix(self):
        """
        Creates a pivot table with Field_of_work as rows and stress levels as columns,
        with counts as values.
        """
        # Create a cross tab of field of work and stress level
        cross_tab = pd.crosstab(
            self.df['Field_of_work'],
            self.df['Stress_Detection'],
            normalize='index'  # Normalize by field to get proportions
        )

        # Create a pivot table: fields as rows, stress levels as columns
        pivot_df = cross_tab

        # Create a sparse matrix from the pivot table
        sparse_matrix = csr_matrix(pivot_df)
        print(f"Sparse Matrix Created! Shape: {sparse_matrix.shape}")
        return sparse_matrix, pivot_df.index

    def calculate_cosine_similarity(self, sparse_matrix, field_names):
        """
        Computes cosine similarity between fields of work based on stress patterns.
        Returns a DataFrame of similarity values.
        """
        similarity_matrix = cosine_similarity(sparse_matrix)
        similarity_df = pd.DataFrame(similarity_matrix, index=field_names, columns=field_names)
        print("Cosine Similarity Matrix (first 5x5 snippet):\n", similarity_df.iloc[:5, :5])
        return similarity_df

    def undirected_weight_graph(self, similarity_df):
        """
        Builds an undirected weighted graph from the similarity DataFrame.
        Filters edges to keep only the top
        """
        G = nx.Graph()

        # Add nodes
        for field in similarity_df.index:
            G.add_node(field)

        # Add edges for nonzero similarities
        for field1 in similarity_df.index:
            edges = 0
            for field2 in similarity_df.index:
                if field1 != field2:  # Ensures that nodes aren't connected to themselves
                    weight_val = similarity_df.loc[field1, field2]
                    if weight_val > 0.0:
                        G.add_edge(field1, field2, weight=weight_val)
                        edges += 1

            # Gets rid of outlier nodes without any edges
            if edges == 0:
                G.remove_node(field1)

        # Calculate edge weight threshold (75th percentile)
        edges = G.edges(data=True)
        weights = [edge[2]['weight'] for edge in edges]
        threshold = np.percentile(weights, 75)

        # Create a new graph with only the strongest edges
        G_filtered = nx.Graph()
        for node in G.nodes():
            G_filtered.add_node(node)

        for u, v, data in edges:
            if data['weight'] >= threshold:
                G_filtered.add_edge(u, v, weight=data['weight'])

        print(f"Graph created with {len(G_filtered.nodes())} nodes and {len(G_filtered.edges())} edges")
        return G_filtered

    def color_nodes_by_stress_level(self, G):
        """
        Colors nodes based on the most common stress level category for each field.
        Uses existing stress level categories from the dataset.
        """
        # Find the most common stress level for each field
        field_stress_counts = self.df.groupby(['Field_of_work', 'Stress_Detection']).size().unstack()

        # Determine the most common stress level for each field
        field_category = {}
        for field in G.nodes():
            if field in field_stress_counts.index:
                # Get the most common stress level
                if not field_stress_counts.empty:
                    most_common = field_stress_counts.loc[field].idxmax()
                    field_category[field] = most_common
                else:
                    field_category[field] = "Unknown"

        # Define color mapping for stress levels in dataset
        color_mapping = {
            "Low": 'green',
            "Medium": 'orange',
            "High": 'red'
        }

        # Assign colors to nodes
        node_colors = [color_mapping[field_category[n]] for n in G.nodes()]

        # Calculate node sizes based on number of people in each field
        field_counts = self.df['Field_of_work'].value_counts()
        node_sizes = [100 + 20 * field_counts.get(field, 0) for field in G.nodes()]

        # Draw the graph
        plt.figure(figsize=(12, 10))
        pos = nx.spring_layout(G, seed=42)

        # Draw edges with width proportional to weight
        edge_weights = [G[u][v]['weight'] * 2 for u, v in G.edges()]
        nx.draw_networkx_edges(G, pos, width=edge_weights, alpha=0.6, edge_color="gray")

        # Draw nodes
        nx.draw_networkx_nodes(G, pos, node_color=node_colors, node_size=node_sizes)

        # Draw labels
        nx.draw_networkx_labels(G, pos, font_size=10, font_color="black")

        # Create legend
        legend_elements = [
            plt.Line2D([0], [0], marker='o', color='w', label=key,
                     markerfacecolor=val, markersize=10)
            for key, val in color_mapping.items()
        ]
        plt.legend(handles=legend_elements, loc="upper right", title="Stress Level")

        plt.title("Field of Work Similarity Network (Based on Stress Patterns)")
        plt.axis('off')
        plt.tight_layout()
        plt.savefig('field_stress_network.png')
        plt.show()

        return field_category, color_mapping


def analyze_stress_by_field(df):
    analyzer = StressFieldAnalyzer(df)

    # Create sparse matrix
    sparse_matrix, field_names = analyzer.create_sparse_matrix()

    # Calculate similarity
    similarity_df = analyzer.calculate_cosine_similarity(sparse_matrix, field_names)

    # Build graph
    G = analyzer.undirected_weight_graph(similarity_df)

    # Visualize graph
    field_category, color_mapping = analyzer.color_nodes_by_stress_level(G)

    # Create heatmap of similarity matrix
    plt.figure(figsize=(12, 8))
    mask = np.zeros_like(similarity_df)
    sns.heatmap(similarity_df, annot=True, cmap='viridis', mask=mask)
    plt.title('Field of Work Similarity Matrix (Based on Stress Patterns)')
    plt.tight_layout()
    plt.savefig('field_similarity_heatmap.png')
    plt.show()

    return similarity_df, G, field_category

# Run the functions
similarity_df, G, field_categories = analyze_stress_by_field(df)

"""# **What are the relationships and interdependencies between various numerical health and lifestyle factors, and how strongly do these factors correlate with each other?**"""

# Change the Stress column into nunmerical values
stress_map = {'Low': 1, 'Medium': 2, 'High': 3}
df['Stress_Detection'] = df['Stress_Detection'].map(stress_map)
# Before splitting, drop rows with NaN
df.dropna(subset=['Stress_Detection'], inplace=True)
# Get only numerical columns for correlation
numerical_columns = df.select_dtypes(include=['float64', 'int64']).columns
print("Numerical columns for correlation:", list(numerical_columns))

# Calculate the correlation matrix
correlation_matrix = df[numerical_columns].corr()

# Visualize the correlation matrix
plt.figure(figsize=(14, 12))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f")
plt.title('Numerical Column Correlation Matrix', fontsize=16)
plt.tight_layout()
plt.savefig('correlation_matrix.png')
plt.show()
#Find top 5 Coorelation values for stress
coorelation_with_stress = correlation_matrix['Stress_Detection'].sort_values(ascending=False)
print(f'The top 5 statistics with the biggest impact on stress are:\n{coorelation_with_stress[1:6]}')

"""# **Using Supervised Learning to Create a Machine Learning for Stress Prediction Based on the Most Impactful Health and Lifestyle Factors**"""

# Select the top 5 features that have the highest correlation with stress
top_features = coorelation_with_stress.index[1:6].tolist()

# Define independent variables, top 5 features with highest correlation to stress, and dependent variable, stress
X = df[top_features]
Y = df['Stress_Detection']

# Create a range of test sizes to evaluate
test_size_range = np.arange(0.1, 1, 0.01)
accuracy_list = []

# Loop through different test sizes to find the optimal test size
for test_size in test_size_range:
  # Split the data into training and testing sets
  X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=test_size, random_state=42)

  # Initialize KNN model with 5 neighbors
  knn = KNeighborsClassifier(n_neighbors=5)

  # Train the model
  knn.fit(X_train, Y_train)

  # Make predictions
  Y_pred = knn.predict(X_test)

  # Calculate accuracy and add to list
  accuracy = accuracy_score(Y_test, Y_pred)
  accuracy_list.append(accuracy)

# Plot accuracy vs test size
plt.figure(figsize=(10, 6))
plt.plot(test_size_range, accuracy_list)
plt.xlabel('Test Size')
plt.ylabel('Accuracy')
plt.title('Accuracy vs Test Size')
plt.grid(True)
plt.show()

# Find the test size that gave the highest accuracy
best_accuracy_index = accuracy_list.index(max(accuracy_list))
optimal_test_size = test_size_range[best_accuracy_index]
print(f'The optimal test size is {optimal_test_size:.2f} with accuracy {max(accuracy_list):.4f}')

# Create a consistent train/test split with the optimal test size for all further testing
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=optimal_test_size, random_state=42)

# Test different numbers of neighbors for KNN
neighbors_range = range(1, 100)
neighbors_accuracy_list = []

# Loop through different neighbor counts using the same train/test split
for n in neighbors_range:
  # Initialize KNN with n neighbors
  knn = KNeighborsClassifier(n_neighbors=n)

  # Train the model
  knn.fit(X_train, Y_train)

  # Make predictions
  Y_pred = knn.predict(X_test)

  # Calculate accuracy and add to list
  accuracy = accuracy_score(Y_test, Y_pred)
  neighbors_accuracy_list.append(accuracy)

# Plot accuracy vs number of neighbors
plt.figure(figsize=(10, 6))
plt.plot(neighbors_range, neighbors_accuracy_list)
plt.xlabel('Number of Neighbors')
plt.ylabel('Accuracy')
plt.title('Accuracy vs Number of Neighbors')
plt.grid(True)
plt.show()

# Find the optimal number of neighbors
best_neighbors_index = neighbors_accuracy_list.index(max(neighbors_accuracy_list))
optimal_neighbors = neighbors_range[best_neighbors_index]
print(f'The optimal number of neighbors is {optimal_neighbors} with accuracy {max(neighbors_accuracy_list):.4f}')

# Train final KNN model with the optimal number of neighbors
knn = KNeighborsClassifier(n_neighbors=optimal_neighbors)
knn.fit(X_train, Y_train)

# Calculate accuracy for KNN
Y_pred = knn.predict(X_test)
knn_accuracy = accuracy_score(Y_test, Y_pred)
print(f"KNN Model Accuracy: {knn_accuracy:.4f}")

# Try Random Forest to potentially improve accuracy
n_estimators_range = range(1, 100, 1)
n_estimator_accuracy_list = []

# Loop through different numbers of estimators (trees) using the same train/test split
for n in n_estimators_range:
  # Initialize Random Forest with n estimators
  rf = RandomForestClassifier(n_estimators=n, random_state=42)

  # Train the model
  rf.fit(X_train, Y_train)

  # Make predictions
  y_pred = rf.predict(X_test)

  # Calculate accuracy and add to list
  accuracy = accuracy_score(Y_test, y_pred)
  n_estimator_accuracy_list.append(accuracy)

# Plot accuracy vs number of estimators
plt.figure(figsize=(10, 6))
plt.plot(n_estimators_range, n_estimator_accuracy_list)
plt.xlabel('Number of Estimators')
plt.ylabel('Accuracy')
plt.title('Accuracy vs Number of Estimators')
plt.grid(True)
plt.show()

# Find the optimal number of estimators
best_estimators_index = n_estimator_accuracy_list.index(max(n_estimator_accuracy_list))
optimal_n_estimators = n_estimators_range[best_estimators_index]
print(f'The optimal number of estimators is {optimal_n_estimators} with accuracy {max(n_estimator_accuracy_list):.4f}')

# Train final Random Forest model with optimal number of estimators
rf = RandomForestClassifier(n_estimators=optimal_n_estimators, random_state=42)
rf.fit(X_train, Y_train)

# Make predictions and calculate accuracy
y_pred = rf.predict(X_test)
rf_accuracy = accuracy_score(Y_test, y_pred)
print(f"Random Forest Model Accuracy: {rf_accuracy:.4f}")

# Compare models and select the best one
if rf_accuracy >= knn_accuracy:
    best_model = rf
    print("Random Forest performed better and will be used for prediction.")
else:
    best_model = knn
    print("KNN performed better and will be used for prediction.")

# Create sample data for a new patient
new_patient_data = {
  'Cholesterol_Level': 180,
  "Blood_Pressure": 180,
  'Blood_Sugar_Level': 120,
  "Screen_Time": 5,
  "Caffeine_Intake": 100
}

# Convert to DataFrame for prediction
new_patient_df = pd.DataFrame([new_patient_data])

# Predict stress level for the new patient using the best model
predicted_stress_level = best_model.predict(new_patient_df)

# Print prediction results with patient information
print("\nPredicted Stress Level:")
print(f"Patient Statistics:")
print(f" Cholesterol: {new_patient_data['Cholesterol_Level']} mg/dl")
print(f" Blood Pressure: {new_patient_data['Blood_Pressure']} mmHg")
print(f" Blood Sugar: {new_patient_data['Blood_Sugar_Level']} mg/dl")
print(f" Screen Time: {new_patient_data['Screen_Time']} hours/day")
print(f" Caffeine Intake: {new_patient_data['Caffeine_Intake']} mg/day")

"""# **What Lifestyle Factors Correlate with Ideal Health Metrics?**"""

# Define ideal ranges for various health metrics
ideal_ranges = {
    "Blood_Sugar_Level": range(1, 121),  # Ideal blood sugar is under 120
    "Sleep_Quality": range(4, 6),        # Ideal sleep quality is 4-5
    "Cholesterol_Level": range(1, 201),  # Ideal cholesterol is under 200
    'Stress_Detection': [1]              # In a range from 1 to 3, 1 is the ideal stress level
}

def find_ideal_stats(df, stat):
    """
    Analyzes data to find ideal metrics for a selected health parameter and provides personalized recommendations.

    Parameters:
    df (pandas.DataFrame): Dataset containing health metrics
    stat (str): The health parameter to optimize (e.g., "Blood_Sugar_Level")
    """
    # Filter the dataset to only include rows where the selected stat is in the ideal range
    if stat == 'Stress_Detection':
        filtered_for_stat = df[df[stat] == ideal_ranges[stat][0]]
    else:
        filtered_for_stat = df[df[stat].isin(ideal_ranges[stat])]

    # Find the top 5 features that correlate most strongly with the selected stat
    top_5_correlations = correlation_matrix[stat].sort_values(ascending=False)
    top_5_features = top_5_correlations.index[1:6].tolist()

    # Calculate the average values of those features for people with ideal levels of the selected stat
    average_metrics = filtered_for_stat[top_5_features].mean().round(2)

    # Display the ideal metrics
    print(f'Ideal Metrics for {stat}:')
    print(average_metrics)

    # Collect the user's current metrics
    user_data = {}
    for metric in top_5_features:
        user_data[metric] = float(input(f"Enter your current value for {metric}: "))

    # Create a DataFrame with the user's data
    user_df = pd.DataFrame([user_data])

    # Generate personalized recommendations by comparing user values to ideal averages
    print(f"\nRecommendations to improve {stat}:")
    for metric, average_value in average_metrics.items():
        if user_df[metric][0] > average_value:
            print(f"Decrease {metric} to around {average_value}")
        else:
            print(f"Your {metric} is good.")

# Prompt the user to select which health parameter they want to improve
stat = input('Enter which of the following you would like to improve: Blood_Sugar_Level, Sleep_Quality, Cholesterol_Level, Stress_Detection: ')

# Call the function to analyze and provide recommendations
find_ideal_stats(df, stat)